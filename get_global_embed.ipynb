{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing global embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create test graph 1'''\n",
    "ccn = defaultdict(list)\n",
    "ccn[3].append(4)\n",
    "ccn[4].append(3)\n",
    "\n",
    "node_client_map = {0:0,1:0,2:0,3:0,4:1,5:1,6:1,7:1}\n",
    "node_assignment = [0,0,0,0,1,1,1,1]\n",
    "embeddings = torch.tensor([\n",
    "                [[[1,1],[2,2],[3,3],[4,4],[0,0],[0,0],[0,0],[0,0]],\n",
    "                 [[2,2],[4,4],[6,6],[8,8],[0,0],[0,0],[0,0],[0,0]],\n",
    "                 [[3,3],[5,5],[7,7],[9,9],[0,0],[0,0],[0,0],[0,0]]],\n",
    "                 \n",
    "                [[[0,0],[0,0],[0,0],[0,0],[1,1],[2,2],[3,3],[4,4]],\n",
    "                [[0,0],[0,0],[0,0],[0,0],[2,2],[4,4],[6,6],[8,8]],\n",
    "                [[0,0],[0,0],[0,0],[0,0],[3,3],[5,5],[7,7],[9,9]]]])\n",
    "adj_list = [[0,2], [1,3], [0,2,3], [1,2,3,4], [3,4,5], [4,5,6],[5,6,7],[6,7]]\n",
    "\n",
    "# '''Create test graph 2'''\n",
    "# ccn = defaultdict(list)\n",
    "# ccn[3].append(4)\n",
    "# ccn[4].append(3)\n",
    "# ccn[3].append(8)\n",
    "# ccn[8].append(3)\n",
    "# node_client_map = {0:0,1:0,2:0,3:0,4:1,5:1,6:1,7:1,8:2,9:2,10:2}\n",
    "# embeddings = torch.tensor([[[[1,1],[2,2],[3,3],[4,4],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],\n",
    "#                  [[2,2],[4,4],[6,6],[8,8],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],\n",
    "#                  [[3,3],[5,5],[7,7],[9,9],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]],\n",
    "\n",
    "#                 [[[0,0],[0,0],[0,0],[0,0],[1,1],[2,2],[3,3],[4,4],[0,0],[0,0],[0,0]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[2,2],[4,4],[6,6],[8,8],[0,0],[0,0],[0,0]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[3,3],[5,5],[7,7],[9,9],[0,0],[0,0],[0,0]]],\n",
    "\n",
    "#                 [[[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[1,1],[2,2],[3,3]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[2,2],[4,4],[6,6]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[3,3],[5,5],[7,7]]],\n",
    "#                 ])\n",
    "# adj_list = [[0,2], [1,3], [0,2,3], [1,2,3,4,8], [3,4,5], [4,5,6],[5,6,7],[6,7],[3,8,9,10],[8,9,10],[8,9,10]]\n",
    "\n",
    "# node_embedding_update_sum(3, ccn_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 3\n",
      "[[3, 2], [3, 0], [4, 1], [4, 0]]\n",
      "node 4\n",
      "[[4, 2], [4, 0], [3, 1], [3, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[tensor([1., 1.]),\n",
       "  tensor([2., 2.]),\n",
       "  tensor([3., 3.]),\n",
       "  tensor([4., 4.]),\n",
       "  tensor([1., 1.]),\n",
       "  tensor([2., 2.]),\n",
       "  tensor([3., 3.]),\n",
       "  tensor([4., 4.])],\n",
       " [tensor([3., 3.]),\n",
       "  tensor([6., 6.]),\n",
       "  tensor([9., 9.]),\n",
       "  tensor([13., 13.]),\n",
       "  tensor([7., 7.]),\n",
       "  tensor([6., 6.]),\n",
       "  tensor([9., 9.]),\n",
       "  tensor([12., 12.])],\n",
       " [tensor([3., 3.]),\n",
       "  tensor([5., 5.]),\n",
       "  tensor([7., 7.]),\n",
       "  tensor([16., 16.]),\n",
       "  tensor([16., 16.]),\n",
       "  tensor([5., 5.]),\n",
       "  tensor([7., 7.]),\n",
       "  tensor([9., 9.])]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def node_embedding_update_sum(start_node, ccn, k):\n",
    "    embeddings_required = []\n",
    "    dq = deque([(start_node, k, {start_node})])\n",
    "    while dq:\n",
    "        node, hop, nodes_visited = dq.popleft()\n",
    "        embeddings_required.append([node, hop])\n",
    "        if hop > 1 and node == start_node:\n",
    "            embeddings_required += [[node, 0]]  * len(ccn[node]) # count the times 1-hop ccn visits itself\n",
    "        elif hop == 1:\n",
    "            embeddings_required += [[node, 0]] # add 0-hop whenever it is visited\n",
    "\n",
    "        for neigh in ccn[node]:\n",
    "            if neigh not in nodes_visited and hop>0:\n",
    "                dq.append((neigh, hop-1, nodes_visited|{neigh}))\n",
    "\n",
    "    return embeddings_required\n",
    "\n",
    "def get_global_embedding(embeddings, ccn, node_client_map):\n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(len(node_client_map)):\n",
    "            node_embdedding_sum = node_embedding_update_sum(node, ccn, hop)\n",
    "            if (node in [3,4]) and (hop == 2):\n",
    "                print(\"node\", node)\n",
    "                print(node_embdedding_sum)\n",
    "            final_embedding = torch.zeros(embeddings[0][0][0].shape)\n",
    "            for update_node, k in node_embdedding_sum:\n",
    "                final_embedding += embeddings[node_client_map[update_node]][k][update_node]\n",
    "            hop_matrix.append(final_embedding)\n",
    "        hop_embeddings.append(hop_matrix)\n",
    "\n",
    "    return hop_embeddings\n",
    "\n",
    "hop_embeddings = get_global_embedding(embeddings, ccn, node_client_map)\n",
    "hop_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrected Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_embedding_update_sum(start_node, ccn, k):\n",
    "    '''\n",
    "    Function to return the contribution of each neighbouring node to start node and its hop embedding\n",
    "    Inputs:\n",
    "    1) start_node -> node we wish to find contribution for next node embedding\n",
    "    2) ccn -> defaultdict(list) of cross client nodes\n",
    "    3) k -> Hop we wish to find embedding of start_node for\n",
    "\n",
    "    Output:\n",
    "    list of tuples corresponding to (node required, hop) for vector embedding update\n",
    "    '''\n",
    "    embeddings_required = []\n",
    "    dq = deque([(start_node, k, {start_node})])\n",
    "    while dq:\n",
    "        node, hop, nodes_visited = dq.popleft()\n",
    "        embeddings_required.append([node, hop])\n",
    "        if hop > 1 and node == start_node:\n",
    "            embeddings_required += [[node, 0]]  * len(ccn[node]) # count the times 1-hop ccn visits itself\n",
    "        elif hop > 1:\n",
    "            embeddings_required += [[node, 0]] # add 0-hop whenever it is visited\n",
    "\n",
    "        for neigh in ccn[node]:\n",
    "            if neigh not in nodes_visited and hop>0:\n",
    "                dq.append((neigh, hop-1, nodes_visited|{neigh}))\n",
    "\n",
    "    return embeddings_required\n",
    "\n",
    "def get_global_embedding(embeddings, ccn, node_client_map, subnodes_union, first_parti_client):\n",
    "    '''\n",
    "    Function to return the global embedding to update the client's local embeddings, using the formula:\n",
    "    1 hop NE of node i => NE1[i] + SUM(NE0[j]) for j in ccn[i]\n",
    "    2 hop NE of node i => NE2[i] + SUM(NE1[j] + NE0[j] + NE0[i]) for j in ccn[i] + SUM(NE0[k]) for k in ccn[j]\n",
    "\n",
    "    Inputs:\n",
    "    1) embeddings -> defaultdict(Tensor) of 0-hop, 1-hop and 2-hop NE of each client\n",
    "    2) ccn -> defaultdict(list) of cross client nodes\n",
    "    3) node_client_map -> the client each node is assigned for training\n",
    "\n",
    "    Output:\n",
    "    list of 0-hop, 1-hop and 2-hop Global NE \n",
    "    '''\n",
    "    if len(embeddings) == 1:\n",
    "        return embeddings[0] # Only one client\n",
    "    \n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(len(node_client_map)):\n",
    "            node_embdedding_sum = node_embedding_update_sum(node, ccn, hop)\n",
    "            final_embedding = torch.zeros(embeddings[first_parti_client][0][0].shape).to(\"cuda:0\")\n",
    "            for update_node, k in node_embdedding_sum:\n",
    "                if update_node in subnodes_union:\n",
    "                    final_embedding += embeddings[node_client_map[update_node]][k][update_node]\n",
    "            hop_matrix.append(final_embedding)\n",
    "        stack = torch.stack(hop_matrix)\n",
    "        hop_embeddings.append(stack)\n",
    "\n",
    "    return hop_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Version using Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def get_node_embedding_needed(start_node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, k):\n",
    "    ''' Return all the (client, node, number of times needed to add) for each hop. '''\n",
    "    if k == 1:\n",
    "        ne_needed = [[] for _ in range(k)] # info needed for hop 0\n",
    "        to_subtract = clients_adj_matrix[node_client_map[start_node]]\n",
    "        adjustment_coefficient = global_adj_matrix[start_node] - to_subtract[start_node]\n",
    "        # Use NumPy's boolean indexing to find where the adjustment coefficient > 0\n",
    "        indices = np.where(adjustment_coefficient > 0)[0]\n",
    "        ne_needed[0] = [(node_client_map[i], i, adjustment_coefficient[i]) for i in indices]\n",
    "\n",
    "    elif k == 2:\n",
    "        ne_needed = [[] for _ in range(k)] # info needed for hop 0, 1\n",
    "        global_two_hop = np.linalg.matrix_power(global_adj_matrix, 2) # Corrected (** 2 is wrong)\n",
    "        to_subtract = np.linalg.matrix_power(clients_adj_matrix[node_client_map[start_node]], 2)\n",
    "        for neigh in ccn[start_node]:\n",
    "            to_subtract[start_node] += clients_adj_matrix[node_client_map[neigh]][neigh] # Correct (have to specify which row)\n",
    "            ne_needed[1].append((node_client_map[neigh], neigh, 1))\n",
    "\n",
    "        adjustment_coefficient = global_two_hop[start_node] - to_subtract[start_node]\n",
    "\n",
    "        # Use NumPy's boolean indexing to find where the adjustment coefficient > 0\n",
    "        indices = np.where(adjustment_coefficient > 0)[0]\n",
    "        ne_needed[0] = [(node_client_map[i], i, adjustment_coefficient[i]) for i in indices]\n",
    "        \n",
    "    return ne_needed # Corrected\n",
    "\n",
    "def fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list):\n",
    "    global_adj_matrix = np.array([[1 if dst in adj_list[src] else 0 for dst in range(len(adj_list))] for src in range(len(adj_list))]) # correct\n",
    "    clients_adj_matrix = []\n",
    "    for client in range(max(node_client_map.values()) + 1): # Correct\n",
    "        client_adj_matrix = np.array([[1 if dst in adj_list[src] and node_client_map[src] == client and node_client_map[dst] == client else 0 for dst in range(len(adj_list))] for src in range(len(adj_list))])\n",
    "        clients_adj_matrix.append(client_adj_matrix)\n",
    "\n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(len(node_client_map)):\n",
    "            if ccn[node] == [] or hop == 0:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "            else:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "                # print(\"hop\", hop, \"node\", node, \"starting emb\", final_embedding)\n",
    "                ne_needed = get_node_embedding_needed(node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, hop)\n",
    "                for hop_needed, tuples in enumerate(ne_needed):\n",
    "                    for client, node, num_times in tuples:\n",
    "                        # if node in subnodes_union:\n",
    "                            # print(f\"hop {hop_needed}: ({client},{node},{num_times}) => {embeddings[client][hop_needed][node]}\")\n",
    "                        final_embedding += embeddings[client][hop_needed][node] * num_times\n",
    "\n",
    "            hop_matrix.append(final_embedding)\n",
    "        hop_embeddings.append(hop_matrix)\n",
    "\n",
    "    return hop_embeddings\n",
    "\n",
    "# hop_embeddings = fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list)\n",
    "# print(\"Final:\", hop_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Sparse Matrix to optimize the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def get_node_embedding_needed(start_node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, k):\n",
    "    ''' Return all the (client, node, number of times needed to add) for each hop. '''\n",
    "    ne_needed = [[] for _ in range(k)]\n",
    "    \n",
    "    if k == 1:\n",
    "        # Convert to dense for the specific row (since sparse slicing is inefficient for single rows)\n",
    "        to_subtract = clients_adj_matrix[node_client_map[start_node]][start_node].toarray()[0]\n",
    "        adjustment_coefficient = global_adj_matrix[start_node].toarray()[0] - to_subtract\n",
    "        indices = np.where(adjustment_coefficient > 0)[0]\n",
    "        ne_needed[0] = [(node_client_map[i], i, adjustment_coefficient[i]) for i in indices]\n",
    "    \n",
    "    elif k == 2:\n",
    "        # Compute global_two_hop using sparse matrix multiplication\n",
    "        global_two_hop = global_adj_matrix @ global_adj_matrix\n",
    "        to_subtract = clients_adj_matrix[node_client_map[start_node]] @ clients_adj_matrix[node_client_map[start_node]]\n",
    "        \n",
    "        # Add contributions from neighbors\n",
    "        start_subtract = to_subtract[start_node].toarray()[0]\n",
    "        for neigh in ccn[start_node]:\n",
    "            start_subtract += clients_adj_matrix[node_client_map[neigh]][neigh].toarray()[0]\n",
    "            ne_needed[1].append((node_client_map[neigh], neigh, 1))\n",
    "        \n",
    "        # Compute adjustment coefficient\n",
    "        adjustment_coefficient = global_two_hop[start_node].toarray()[0] - start_subtract\n",
    "        indices = np.where(adjustment_coefficient > 0)[0]\n",
    "        ne_needed[0] = [(node_client_map[i], i, adjustment_coefficient[i]) for i in indices]\n",
    "    \n",
    "    return ne_needed\n",
    "\n",
    "def fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list):\n",
    "    num_nodes = len(adj_list)\n",
    "    num_clients = max(node_client_map.values()) + 1\n",
    "    \n",
    "    # Create global adjacency matrix as a sparse matrix\n",
    "    global_adj_matrix = lil_matrix((num_nodes, num_nodes), dtype=int)\n",
    "    for src in range(num_nodes):\n",
    "        for dst in adj_list[src]:\n",
    "            global_adj_matrix[src, dst] = 1\n",
    "    global_adj_matrix = global_adj_matrix.tocsr()  # Convert to CSR format for efficient operations\n",
    "    \n",
    "    # Create client adjacency matrices as sparse matrices\n",
    "    clients_adj_matrix = []\n",
    "    for client in range(num_clients):\n",
    "        client_adj_matrix = lil_matrix((num_nodes, num_nodes), dtype=int)\n",
    "        for src in range(num_nodes):\n",
    "            if node_client_map[src] == client:\n",
    "                for dst in adj_list[src]:\n",
    "                    if node_client_map[dst] == client:\n",
    "                        client_adj_matrix[src, dst] = 1\n",
    "        clients_adj_matrix.append(client_adj_matrix.tocsr())  # Convert to CSR format\n",
    "    \n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(num_nodes):\n",
    "            if ccn[node] == [] or hop == 0:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "            else:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "                ne_needed = get_node_embedding_needed(node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, hop)\n",
    "                for hop_needed, tuples in enumerate(ne_needed):\n",
    "                    for client, node_needed, num_times in tuples:\n",
    "                        final_embedding += embeddings[client][hop_needed][node_needed] * num_times\n",
    "            hop_matrix.append(final_embedding)\n",
    "        hop_embeddings.append(hop_matrix)\n",
    "    \n",
    "    return hop_embeddings\n",
    "\n",
    "hop_embeddings = fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list)\n",
    "# hop_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensor Sparse to further optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sparse_to_torch_gpu(sparse_matrix):\n",
    "        '''Convert scipy.sparse.csr_matrix to torch.sparse tensor on GPU.'''\n",
    "        sparse_matrix = sparse_matrix.tocoo()\n",
    "        indices = np.vstack((sparse_matrix.row, sparse_matrix.col))\n",
    "        indices = torch.tensor(indices, dtype=torch.long, device='cuda')\n",
    "        values = torch.tensor(sparse_matrix.data, dtype=torch.float32, device='cuda')\n",
    "        shape = sparse_matrix.shape\n",
    "        return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "def get_node_embedding_needed(start_node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, k):\n",
    "    ''' Return all the (client, node, number of times needed to add) for each hop. '''\n",
    "    ne_needed = [[] for _ in range(k)]\n",
    "    \n",
    "    if k == 1:\n",
    "        # Convert to dense for the specific row (since sparse slicing is inefficient for single rows)\n",
    "        to_subtract = clients_adj_matrix[node_client_map[start_node]][start_node].to_dense()[0]\n",
    "        adjustment_coefficient = global_adj_matrix[start_node].to_dense()[0] - to_subtract\n",
    "        indices = torch.where(adjustment_coefficient > 0)[0].cpu().numpy()\n",
    "        ne_needed[0] = [(node_client_map[i], i, adjustment_coefficient[i].item()) for i in indices]\n",
    "    \n",
    "    elif k == 2:\n",
    "        # Compute global_two_hop using sparse matrix multiplication\n",
    "        global_two_hop = torch.sparse.mm(global_adj_matrix, global_adj_matrix)\n",
    "        to_subtract = torch.sparse.mm(clients_adj_matrix[node_client_map[start_node]], clients_adj_matrix[node_client_map[start_node]])\n",
    "        \n",
    "        # Add contributions from neighbors\n",
    "        start_subtract = to_subtract[start_node].to_dense()[0]\n",
    "        for neigh in ccn[start_node]:\n",
    "            start_subtract += clients_adj_matrix[node_client_map[neigh]][neigh].to_dense()[0]\n",
    "            ne_needed[1].append((node_client_map[neigh], neigh, 1))\n",
    "        \n",
    "        # Compute adjustment coefficient\n",
    "        adjustment_coefficient = global_two_hop[start_node].to_dense()[0] - start_subtract\n",
    "        indices = torch.where(adjustment_coefficient > 0)[0].cpu().numpy()\n",
    "        ne_needed[0] = [(node_client_map[i], i, adjustment_coefficient[i].item()) for i in indices]\n",
    "    \n",
    "    return ne_needed\n",
    "\n",
    "def fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list):\n",
    "    num_nodes = len(adj_list)\n",
    "    num_clients = max(node_client_map.values()) + 1\n",
    "    \n",
    "    # Create global adjacency matrix as a sparse matrix\n",
    "    global_adj_matrix = lil_matrix((num_nodes, num_nodes), dtype=int)\n",
    "    for src in range(num_nodes):\n",
    "        for dst in adj_list[src]:\n",
    "            global_adj_matrix[src, dst] = 1\n",
    "    global_adj_matrix = global_adj_matrix.tocsr()  # Convert to CSR format for efficient operations\n",
    "\n",
    "    global_adj_matrix_gpu = _sparse_to_torch_gpu(global_adj_matrix)\n",
    "    \n",
    "    # Create client adjacency matrices as sparse matrices\n",
    "    clients_adj_matrix = []\n",
    "    for client in range(num_clients):\n",
    "        client_adj_matrix = lil_matrix((num_nodes, num_nodes), dtype=int)\n",
    "        for src in range(num_nodes):\n",
    "            if node_client_map[src] == client:\n",
    "                for dst in adj_list[src]:\n",
    "                    if node_client_map[dst] == client:\n",
    "                        client_adj_matrix[src, dst] = 1\n",
    "        clients_adj_matrix.append(client_adj_matrix.tocsr())  # Convert to CSR format\n",
    "\n",
    "    clients_adj_matrix_gpu = _sparse_to_torch_gpu(clients_adj_matrix)\n",
    "    \n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(num_nodes):\n",
    "            if ccn[node] == [] or hop == 0:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "            else:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "                ne_needed = get_node_embedding_needed(node, global_adj_matrix_gpu, clients_adj_matrix_gpu, ccn, node_client_map, hop)\n",
    "                for hop_needed, tuples in enumerate(ne_needed):\n",
    "                    for client, node_needed, num_times in tuples:\n",
    "                        final_embedding += embeddings[client][hop_needed][node_needed] * num_times\n",
    "            hop_matrix.append(final_embedding)\n",
    "        hop_embeddings.append(hop_matrix)\n",
    "    \n",
    "    return hop_embeddings\n",
    "\n",
    "hop_embeddings = fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list)\n",
    "# hop_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
