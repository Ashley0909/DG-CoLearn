{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing global embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create test graph 1'''\n",
    "ccn = defaultdict(list)\n",
    "ccn[3].append(4)\n",
    "ccn[4].append(3)\n",
    "\n",
    "node_client_map = {0:0,1:0,2:0,3:0,4:1,5:1,6:1,7:1}\n",
    "embeddings = torch.tensor([\n",
    "                [[[1,1],[2,2],[3,3],[4,4],[0,0],[0,0],[0,0],[0,0]],\n",
    "                 [[2,2],[4,4],[6,6],[8,8],[0,0],[0,0],[0,0],[0,0]],\n",
    "                 [[3,3],[5,5],[7,7],[9,9],[0,0],[0,0],[0,0],[0,0]]],\n",
    "                 \n",
    "                [[[0,0],[0,0],[0,0],[0,0],[1,1],[2,2],[3,3],[4,4]],\n",
    "                [[0,0],[0,0],[0,0],[0,0],[2,2],[4,4],[6,6],[8,8]],\n",
    "                [[0,0],[0,0],[0,0],[0,0],[3,3],[5,5],[7,7],[9,9]]]])\n",
    "adj_list = [[0,2], [1,3], [0,2,3], [1,2,3,4], [3,4,5], [4,5,6],[5,6,7],[6,7]]\n",
    "\n",
    "# node_embedding_update_sum(4, ccn_1, 0)\n",
    "# get_global_embedding(embeddings_1, ccn_1, node_assignment_1)\n",
    "\n",
    "# '''Create test graph 2'''\n",
    "# ccn = defaultdict(list)\n",
    "# ccn[3].append(4)\n",
    "# ccn[4].append(3)\n",
    "# ccn[3].append(8)\n",
    "# ccn[8].append(3)\n",
    "# node_client_map = {0:0,1:0,2:0,3:0,4:1,5:1,6:1,7:1,8:2,9:2,10:2}\n",
    "# embeddings = torch.tensor([[[[1,1],[2,2],[3,3],[4,4],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],\n",
    "#                  [[2,2],[4,4],[6,6],[8,8],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],\n",
    "#                  [[3,3],[5,5],[7,7],[9,9],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]],\n",
    "\n",
    "#                 [[[0,0],[0,0],[0,0],[0,0],[1,1],[2,2],[3,3],[4,4],[0,0],[0,0],[0,0]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[2,2],[4,4],[6,6],[8,8],[0,0],[0,0],[0,0]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[3,3],[5,5],[7,7],[9,9],[0,0],[0,0],[0,0]]],\n",
    "\n",
    "#                 [[[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[1,1],[2,2],[3,3]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[2,2],[4,4],[6,6]],\n",
    "#                 [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[3,3],[5,5],[7,7]]],\n",
    "#                 ])\n",
    "# adj_list = [[0,2], [1,3], [0,2,3], [1,2,3,4,8], [3,4,5], [4,5,6],[5,6,7],[6,7],[3,8,9,10],[8,9,10],[8,9,10]]\n",
    "\n",
    "# node_embedding_update_sum(3, ccn_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_embedding_update_sum(start_node, ccn, k):\n",
    "    embeddings_required = []\n",
    "    dq = deque([(start_node, k, {start_node})])\n",
    "    while dq:\n",
    "        node, hop, nodes_visited = dq.popleft()\n",
    "        embeddings_required.append([node, hop])\n",
    "        if hop > 1 and node == start_node:\n",
    "            embeddings_required += [[node, 0]]  * len(ccn[node]) # count the times 1-hop ccn visits itself\n",
    "        elif hop > 1:\n",
    "            embeddings_required += [[node, 0]] # add 0-hop whenever it is visited\n",
    "\n",
    "        for neigh in ccn[node]:\n",
    "            if neigh not in nodes_visited and hop>0:\n",
    "                dq.append((neigh, hop-1, nodes_visited|{neigh}))\n",
    "\n",
    "    return embeddings_required\n",
    "\n",
    "def get_global_embedding(embeddings, ccn, node_client_map):\n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(len(node_client_map)):\n",
    "            node_embdedding_sum = node_embedding_update_sum(node, ccn, hop)\n",
    "            final_embedding = torch.zeros(embeddings[0][0][0].shape)\n",
    "            for update_node, k in node_embdedding_sum:\n",
    "                final_embedding += embeddings[node_client_map[update_node]][k][update_node]\n",
    "            hop_matrix.append(final_embedding)\n",
    "        hop_embeddings.append(hop_matrix)\n",
    "\n",
    "    return hop_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrected Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_embedding_update_sum(start_node, ccn, k):\n",
    "    '''\n",
    "    Function to return the contribution of each neighbouring node to start node and its hop embedding\n",
    "    Inputs:\n",
    "    1) start_node -> node we wish to find contribution for next node embedding\n",
    "    2) ccn -> defaultdict(list) of cross client nodes\n",
    "    3) k -> Hop we wish to find embedding of start_node for\n",
    "\n",
    "    Output:\n",
    "    list of tuples corresponding to (node required, hop) for vector embedding update\n",
    "    '''\n",
    "    embeddings_required = []\n",
    "    dq = deque([(start_node, k, {start_node})])\n",
    "    while dq:\n",
    "        node, hop, nodes_visited = dq.popleft()\n",
    "        embeddings_required.append([node, hop])\n",
    "        if hop > 1 and node == start_node:\n",
    "            embeddings_required += [[node, 0]]  * len(ccn[node]) # count the times 1-hop ccn visits itself\n",
    "        elif hop > 1:\n",
    "            embeddings_required += [[node, 0]] # add 0-hop whenever it is visited\n",
    "\n",
    "        for neigh in ccn[node]:\n",
    "            if neigh not in nodes_visited and hop>0:\n",
    "                dq.append((neigh, hop-1, nodes_visited|{neigh}))\n",
    "\n",
    "    return embeddings_required\n",
    "\n",
    "def get_global_embedding(embeddings, ccn, node_client_map, subnodes_union, first_parti_client):\n",
    "    '''\n",
    "    Function to return the global embedding to update the client's local embeddings, using the formula:\n",
    "    1 hop NE of node i => NE1[i] + SUM(NE0[j]) for j in ccn[i]\n",
    "    2 hop NE of node i => NE2[i] + SUM(NE1[j] + NE0[j] + NE0[i]) for j in ccn[i] + SUM(NE0[k]) for k in ccn[j]\n",
    "\n",
    "    Inputs:\n",
    "    1) embeddings -> defaultdict(Tensor) of 0-hop, 1-hop and 2-hop NE of each client\n",
    "    2) ccn -> defaultdict(list) of cross client nodes\n",
    "    3) node_client_map -> the client each node is assigned for training\n",
    "\n",
    "    Output:\n",
    "    list of 0-hop, 1-hop and 2-hop Global NE \n",
    "    '''\n",
    "    if len(embeddings) == 1:\n",
    "        return embeddings[0] # Only one client\n",
    "    \n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(len(node_client_map)):\n",
    "            node_embdedding_sum = node_embedding_update_sum(node, ccn, hop)\n",
    "            final_embedding = torch.zeros(embeddings[first_parti_client][0][0].shape).to(\"cuda:0\")\n",
    "            for update_node, k in node_embdedding_sum:\n",
    "                if update_node in subnodes_union:\n",
    "                    final_embedding += embeddings[node_client_map[update_node]][k][update_node]\n",
    "            hop_matrix.append(final_embedding)\n",
    "        stack = torch.stack(hop_matrix)\n",
    "        hop_embeddings.append(stack)\n",
    "\n",
    "    return hop_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Version using Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hop 1 node 3 starting emb tensor([8, 8])\n",
      "hop 0: (1,4,1) => tensor([1, 1])\n",
      "hop 1 node 4 starting emb tensor([2, 2])\n",
      "hop 0: (0,3,1) => tensor([4, 4])\n",
      "hop 2 node 3 starting emb tensor([9, 9])\n",
      "hop 0: (0,3,1) => tensor([4, 4])\n",
      "hop 0: (1,4,1) => tensor([1, 1])\n",
      "hop 1: (1,4,1) => tensor([2, 2])\n",
      "hop 2 node 4 starting emb tensor([3, 3])\n",
      "hop 0: (0,3,1) => tensor([4, 4])\n",
      "hop 0: (1,4,1) => tensor([1, 1])\n",
      "hop 1: (0,3,1) => tensor([8, 8])\n",
      "Final: [[tensor([1, 1]), tensor([2, 2]), tensor([3, 3]), tensor([4, 4]), tensor([1, 1]), tensor([2, 2]), tensor([3, 3]), tensor([4, 4])], [tensor([2, 2]), tensor([4, 4]), tensor([6, 6]), tensor([9, 9]), tensor([6, 6]), tensor([4, 4]), tensor([6, 6]), tensor([8, 8])], [tensor([3, 3]), tensor([5, 5]), tensor([7, 7]), tensor([16, 16]), tensor([16, 16]), tensor([5, 5]), tensor([7, 7]), tensor([9, 9])]]\n"
     ]
    }
   ],
   "source": [
    "def get_node_embedding_needed(start_node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, k):\n",
    "    ''' Return all the (client, node, number of times needed to add) for each hop. '''\n",
    "    if k == 1:\n",
    "        ne_needed = [[] for _ in range(k)] # info needed for hop 0\n",
    "        to_subtract = clients_adj_matrix[node_client_map[start_node]]\n",
    "        adjustment_coefficient = global_adj_matrix[start_node] - to_subtract[start_node]\n",
    "        for i, coe in enumerate(adjustment_coefficient):\n",
    "            if coe > 0:\n",
    "                ne_needed[0].append((node_client_map[i], i, coe))\n",
    "\n",
    "    elif k == 2:\n",
    "        ne_needed = [[] for _ in range(k)] # info needed for hop 0, 1\n",
    "        global_two_hop = np.linalg.matrix_power(global_adj_matrix, 2) # Corrected (** 2 is wrong)\n",
    "        to_subtract = np.linalg.matrix_power(clients_adj_matrix[node_client_map[start_node]], 2)\n",
    "        for neigh in ccn[start_node]:\n",
    "            to_subtract[start_node] += clients_adj_matrix[node_client_map[neigh]][neigh] # Correct (have to specify which row)\n",
    "            ne_needed[1].append((node_client_map[neigh], neigh, 1))\n",
    "\n",
    "        adjustment_coefficient = global_two_hop[start_node] - to_subtract[start_node]\n",
    "        for i, coe in enumerate(adjustment_coefficient):\n",
    "            if coe > 0:\n",
    "                ne_needed[0].append((node_client_map[i], i, coe))\n",
    "        \n",
    "    return ne_needed # Corrected\n",
    "\n",
    "def fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list, subnodes_union):\n",
    "    global_adj_matrix = np.array([[1 if dst in adj_list[src] else 0 for dst in range(len(adj_list))] for src in range(len(adj_list))]) # correct\n",
    "    clients_adj_matrix = []\n",
    "    for client in range(max(node_client_map.values()) + 1): # Correct\n",
    "        client_adj_matrix = np.array([[1 if dst in adj_list[src] and node_client_map[src] == client and node_client_map[dst] == client else 0 for dst in range(len(adj_list))] for src in range(len(adj_list))])\n",
    "        clients_adj_matrix.append(client_adj_matrix)\n",
    "\n",
    "    hop_embeddings = []\n",
    "    for hop in range(3):\n",
    "        hop_matrix = []\n",
    "        for node in range(len(node_client_map)):\n",
    "            if ccn[node] == [] or hop == 0:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "            else:\n",
    "                final_embedding = embeddings[node_client_map[node]][hop][node].clone()\n",
    "                # print(\"hop\", hop, \"node\", node, \"starting emb\", final_embedding)\n",
    "                ne_needed = get_node_embedding_needed(node, global_adj_matrix, clients_adj_matrix, ccn, node_client_map, hop)\n",
    "                for hop_needed, tuples in enumerate(ne_needed):\n",
    "                    for client, node, num_times in tuples:\n",
    "                        if node in subnodes_union:\n",
    "                            # print(f\"hop {hop_needed}: ({client},{node},{num_times}) => {embeddings[client][hop_needed][node]}\")\n",
    "                            final_embedding += embeddings[client][hop_needed][node] * num_times\n",
    "\n",
    "            hop_matrix.append(final_embedding)\n",
    "        hop_embeddings.append(hop_matrix)\n",
    "\n",
    "    return hop_embeddings\n",
    "\n",
    "hop_embeddings = fast_get_global_embedding(embeddings, ccn, node_client_map, adj_list)\n",
    "# print(\"Final:\", hop_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
